---
title: "1023_LSS_kobayashi"
output: html_document
date: "2023-10-23"
---
# clean up workspace
```{r}
rm(list = ls())
```

# load required packages
```{r}
library(tidyverse) # data handling and visualization
library(rvest) # scraping
library(tm) # text mining
library(RMeCab) # tokenization
library(tidytext) # text mining using tidy tools
library(lsa) # LSA
library(LSAfun) # applied LSA functions
library(ggplot2) # R tools for plot
library(topicmodels) # LDA and CTM
library(LDAvis) # visualization for LDA
library(wordVectors) # word2vec
library(tsne) # t-SNE
library(readr) # separate csv file's row
library(dplyr)# tibble function
library(rsvd)# rsvd
library(caret)　# normalize vector
``` 

# define corpus
```{r}
fuman_corpus <- VCorpus(DirSource(dir = "/home/mkobayashi/taniyama/fuman/test_fuman.csv_1/fuman_csv_1_1000"))
fuman_corpus
inspect(fuman_corpus[[1]])
```

# expand corpus into tibble
```{r}
dat <- tidy(fuman_corpus) # expand corpus object into tibble
```


# convert character encoding from utf-8 to shift-jis
```{r}
dat2 <- dat %>% as.data.frame() # character encoding is utf-8 at this point
write.csv(dat2, "dat2.csv", row.names = F)
dat3 <- read.csv("dat2.csv", header = T) # character encoding is now shift-jis
```



# morphological analysis
```{r}
df_id4 <- dat3 %>% RMeCabDF("text", 1)
df_id_tokens_0 <- purrr::pmap_df(
  list(
    nv = df_id4,
    doc = dat2$id
  ),
  function(nv, doc) {
    tibble(
      doc = doc,
      term = nv,
      PoS = names(nv)
    )
  }
)
# make data tidy
tail(df_id_tokens_0, 40)
```


# morphological analysis
```{r}
# stopword
stopword_jp <- read.csv("/home/mkobayashi/taniyama/fuman/watanabe.stop_words.csv", header = FALSE, fileEncoding = "UTF-8-BOM") 
stopword_jp_add <- data.frame(V1 = c('""','"',"x","・","：","；","／","、","。",",",".","（","）","〔","〕",
                                     "［","］","｛","｝","〈","〉","《","》","「","」","『","』","【","】","‘","’","“","”",
         "―","‐","…","＄","￠","￡","￥"))# add "x" by koba
stopword_jp <- bind_rows(stopword_jp, stopword_jp_add)
colnames(stopword_jp) <- "term"

df_id_tokens_1 <- df_id_tokens_0 %>%
  anti_join(stopword_jp, by = "term") 

df_id_tokens_2 <- df_id_tokens_1 %>%filter(PoS %in% c("名詞", "形容詞")) 

df_id_tokens_3 <- df_id_tokens_2 %>%group_by(doc, term) 

df_id_tokens_4 <- df_id_tokens_3%>%summarise(count = n()) %>% ungroup()
# remove stop words, restrict PoS to nouns and adjectives, and count selected words

# form document-feature matrices excluding features that occur less than 10 times in the corpora.
# df_id_tokens_5 <- df_id_tokens_4 [df_id_tokens_4 $count >= 10, ]

# tail(df_id_tokens_5, 40)
```

# convert frequency table into DTM
```{r}
DTM_id <- cast_dtm(df_id_tokens_4, document = "doc", term = "term", value = "count") %>% as.matrix()
DTM_id[6,5]# Optionally, we can show the content of the matrix DTM by just typing the dataset name
str(DTM_id)# 1000(doc)×4040(term)
```


# LSA
```{r}
myLSAspace <- lsa(t(DTM_id), dims = dimcalc_share())
dim(myLSAspace$tk)# Check how many rows/columns the tk matrix has
# 4040(term)×268(dimension)
# row_names <- rownames(myLSAspace$tk)
# row_names
```


# calculate similarities between words
```{r}

# Obtain right singular vector.
# V_rsvd <- t(fuman_rsvd$v)
# dim(V_rsvd) #300(dim)×4040(term)
# head(V_rsvd)
# V_rsvd[4,]

V_lsa <- t(myLSAspace$tk)
dim(V_lsa) #265(dim)×4041(term)
head(V_lsa)
V_lsa[4,]


## normalization
# Function to normalize a matrix of orthogonal bases
  # Normalize each column
  normalizeOrthogonalMatrix <- function(orthogonal_matrix) {
  # Get the number of columns in the matrix
  num_cols <- ncol(orthogonal_matrix)

  # Get column name
  col_names <- colnames(orthogonal_matrix)

  # Normalize each column
  normalized_matrix <- matrix(0, nrow = nrow(orthogonal_matrix), ncol = num_cols)
  for (i in 1:num_cols) {
    column <- orthogonal_matrix[, i]
    normalized_column <- column / sqrt(sum(column^2))
    normalized_matrix[, i] <- normalized_column
  }

  # Set original column name to normalized matrix
  colnames(normalized_matrix) <- col_names

  return(normalized_matrix)
  }


normalized_matrix.fuman_lsa<- normalizeOrthogonalMatrix(V_lsa)
normalized_matrix.fuman_lsa
dim(normalized_matrix.fuman_lsa) #265(dim)×4041(term)

# using normalize function
function_normalizeMatrix_lsa <- normalize(V_lsa)
```


# automatic generation of a dictionary of polarity(rsvd)
```{r}
# one_to_minus_one <- function(vec){
#   zero_to_one <- (vec-min(vec)) / (max(vec) - min(vec))
#   return(2 * zero_to_one - 1)
# } # define a scaling function

worst <- c("悪い","意地悪","かわいそう","否定","残念","間違い","ダメ") 
best <- c("良い","最高","素敵","幸せ","正しい","明るい")
polars_slice <- normalized_matrix.fuman_lsa[,c(worst,best)]
polars_slice
dim(normalized_matrix.fuman_lsa) #265(dim)×4041(terms)
dim(polars_slice) #265(dim)×13(polarity terms)

# confirm matrix
dat <- t(t(polars_slice) %*% normalized_matrix.fuman_lsa) %>% as.data.frame
dim(dat)# 4040(all terms)×13(polarity terms)


# Check by extracting one
sample1 <-dat["良い",] 
sample1

--------------------------------------------------------------------------------
# compare the result 
polars_slice2 <- function_normalizeMatrix_lsa[,c(worst,best)]
dim(polars_slice2)

dat2 <- t(t(polars_slice2) %*% function_normalizeMatrix_lsa) %>% as.data.frame
dim(dat2)# 4041(all terms)×13(polarity terms)

# Check by extracting one
sample2 <-dat2["良い",] 
sample2
--------------------------------------------------------------------------------

autodic_watanabe <- t(t(polars_slice) %*% normalized_matrix.fuman_lsa) %>% 
  as.data.frame() %>% 
  mutate(across(1:7, ~ . * -1)) %>% 
  mutate(across(8:13, ~ . * 1)) %>% 
  mutate(polarity = rowMeans(.))

word <- list("最悪","良い","素敵","心配","コーヒー")


result <- autodic_watanabe[c("最悪","良い","素敵","心配","コーヒー"),] %>% as.data.frame()
result 
```

# polarity value of the document
```{r}

# テキストを形態素解析
text1 <- RMeCabFreq("/home/mkobayashi/taniyama/fuman/test_fuman.csv_1/fuman_csv_1_1000/row_5_1.csv")
str(text1)

# get autodic_watanabe row names
row_names <- rownames(autodic_watanabe)

# 感情極性値を格納する変数を初期化
sentiment_values <- numeric()

# 各単語の感情極性値を計算
for (word in text1$Term) {
  # wordがautodic_watanabeに存在するか確認
  if (word %in% row_names) {
    # 単語の感情極性値を取得
    row_index <- which(row_names == word)  # wordが存在する行のインデックスを取得
    polarity <- autodic_watanabe[row_index, "polarity"]  # 行 index に対応するpolarity列の値を取得
    
    # 単語の出現回数を取得
    word_count <- text1$Freq[text1$Term == word]
    
    # 頻出度で重み付け
    weighted_polarity <- polarity * word_count
    sentiment_values <- c(sentiment_values, weighted_polarity)
  } else {
    # 単語がautodic_watanabeに存在しない場合、感情極性値を0とする
    sentiment_values <- c(sentiment_values, 0)
  }
}
sentiment_values 

# 文章全体の感情極性値を計算
sentiment_score <- sum(sentiment_values) / sum(text1$Freq)

# 結果を出力
cat("文章全体の感情極性値:", sentiment_score, "\n")

```