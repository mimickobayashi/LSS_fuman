---
title: "fuman_tentative1013"
output: html_document
date: "2023-10-13"
---

# clean up workspace
```{r}
rm(list = ls())
```

# load required packages
```{r}
library(tidyverse) # data handling and visualization
library(rvest) # scraping
library(tm) # text mining
library(RMeCab) # tokenization
library(tidytext) # text mining using tidy tools
library(lsa) # LSA
library(LSAfun) # applied LSA functions
library(ggplot2) # R tools for plot
library(topicmodels) # LDA and CTM
library(LDAvis) # visualization for LDA
library(wordVectors) # word2vec
library(tsne) # t-SNE
library(readr) # separate csv file's row
library(dplyr)# tibble function
library(rsvd)# rsvd
install.packages("caret")
library(caret)　# normalize vector
``` 

# define corpus
```{r}
fuman_corpus <- VCorpus(DirSource(dir = "/home/mkobayashi/taniyama/fuman/test_fuman.csv_1/fuman_csv_1_1000"))
fuman_corpus
inspect(fuman_corpus[[1]])
```

# expand corpus into tibble
```{r}
dat <- tidy(fuman_corpus) # expand corpus object into tibble
```


# convert character encoding from utf-8 to shift-jis
```{r}
dat2 <- dat %>% as.data.frame() # character encoding is utf-8 at this point
write.csv(dat2, "dat2.csv", row.names = F)
dat3 <- read.csv("dat2.csv", header = T) # character encoding is now shift-jis
```



# morphological analysis
```{r}
df_id4 <- dat3 %>% RMeCabDF("text", 1)
df_id_tokens_0 <- purrr::pmap_df(
  list(
    nv = df_id4,
    doc = dat2$id
  ),
  function(nv, doc) {
    tibble(
      doc = doc,
      term = nv,
      PoS = names(nv)
    )
  }
)
# make data tidy
tail(df_id_tokens_0, 40)
```


# morphological analysis
```{r}
# stopword
stopword_jp <- read.csv("/home/mkobayashi/taniyama/fuman/watanabe.stop_words.csv", header = FALSE, fileEncoding = "UTF-8-BOM") 
stopword_jp_add <- data.frame(V1 = c('""','"',"x","・","：","；","／","、","。",",",".","（","）","〔","〕",
                                     "［","］","｛","｝","〈","〉","《","》","「","」","『","』","【","】","‘","’","“","”",
         "―","‐","…","＄","￠","￡","￥"))# add "x" by koba
stopword_jp <- bind_rows(stopword_jp, stopword_jp_add)
colnames(stopword_jp) <- "term"

df_id_tokens_1 <- df_id_tokens_0 %>%
  anti_join(stopword_jp, by = "term") 

df_id_tokens_2 <- df_id_tokens_1 %>%filter(PoS %in% c("名詞", "形容詞")) 

df_id_tokens_3 <- df_id_tokens_2 %>%group_by(doc, term) 

df_id_tokens_4 <- df_id_tokens_3%>%summarise(count = n()) %>% ungroup()
# remove stop words, restrict PoS to nouns and adjectives, and count selected words

# form document-feature matrices excluding features that occur less than 10 times in the corpora.
# df_id_tokens_5 <- df_id_tokens_4 [df_id_tokens_4 $count >= 10, ]

# tail(df_id_tokens_5, 40)
```

# convert frequency table into DTM
```{r}
DTM_id <- cast_dtm(df_id_tokens_4, document = "doc", term = "term", value = "count") %>% as.matrix()
DTM_id[6,5]# Optionally, we can show the content of the matrix DTM by just typing the dataset name
str(DTM_id)# 1000(doc)×4040(term)
# col_names <- colnames(DTM_id)
# col_names
```

# LSA
```{r}
myLSAspace <- lsa(t(DTM_id), dims = dimcalc_share())
dim(myLSAspace$tk)# Check how many rows/columns the tk matrix has
# 4040(term)×268(dimension)
# row_names <- rownames(myLSAspace$tk)
# row_names
```

# rsvd
```{r}
fuman_rsvd <- rsvd(DTM_id, k=300,  p = 10, q = 2, sdist = "normal")

# Specify to 300 dimensions and restore the rsvd matrices.
fuman.re <- fuman_rsvd$u %*% diag(fuman_rsvd$d) %*% t(fuman_rsvd$v)

# 正規化平均二乗誤差(画像の再構成の質に関する値)
# The normalized root mean squared error
nrmse <- sqrt(sum((DTM_id - fuman.re) ** 2) / sum(DTM_id** 2))
nrmse
# [1] 0.4963821
```

# calculate similarities between words
```{r}

# Obtain right singular vector.
# V_rsvd <- t(fuman_rsvd$v)
# dim(V_rsvd) #300(dim)×4040(term)
# head(V_rsvd)
# V_rsvd[4,]

V_lsa <- t(myLSAspace$tk)
dim(V_lsa) #265(dim)×4041(term)
head(V_lsa)
V_lsa[4,]


## normalization
# Function to normalize a matrix of orthogonal bases
  # Normalize each column
  normalizeOrthogonalMatrix <- function(orthogonal_matrix) {
  # Get the number of columns in the matrix
  num_cols <- ncol(orthogonal_matrix)

  # Get column name
  col_names <- colnames(orthogonal_matrix)

  # Normalize each column
  normalized_matrix <- matrix(0, nrow = nrow(orthogonal_matrix), ncol = num_cols)
  for (i in 1:num_cols) {
    column <- orthogonal_matrix[, i]
    normalized_column <- column / sqrt(sum(column^2))
    normalized_matrix[, i] <- normalized_column
  }

  # Set original column name to normalized matrix
  colnames(normalized_matrix) <- col_names

  return(normalized_matrix)
}


normalized_matrix.fuman_lsa<- normalizeOrthogonalMatrix(V_lsa)
normalized_matrix.fuman_lsa
dim(normalized_matrix.fuman_lsa) #265(dim)×4041(term)


# Compute the inner product of orthognomal matrices
# inner_product_matrix_fuman_rsvd<- t(normalized_matrix.fuman_rsvd) %*%normalized_matrix.fuman_rsvd
# inner_product_matrix_fuman_rsvd

```

# automatic generation of a dictionary of polarity(rsvd)
```{r}
# one_to_minus_one <- function(vec){
#   zero_to_one <- (vec-min(vec)) / (max(vec) - min(vec))
#   return(2 * zero_to_one - 1)
# } # define a scaling function

worst <- c("悪い","意地悪","かわいそう","否定","残念","間違い","ダメ") 
best <- c("良い","最高","素敵","幸せ","正しい","明るい")
polars_slice <- normalized_matrix.fuman_lsa[,c(worst,best)]
polars_slice
dim(normalized_matrix.fuman_lsa) #265(dim)×4041(terms)
dim(polars_slice) #265(dim)×13(polarity terms)

# confirm matrix
dat <- t(t(polars_slice) %*% normalized_matrix.fuman_lsa) %>% as.data.frame
dim(dat)# 4040(all terms)×13(polarity terms)

# Check by extracting one. Looks like it's done.
sample1 <-dat["良い",] 
sample1

sample4<-dat["良い",] %>% mutate(polarity = rowSums(.)) 
sample2

sample3<-dat["良い",] %>% mutate(polarity= as.matrix(.)%% c(-1,1))
sample2

# Maybe this code is correct.But I forgot what this code calculated want.
# autodic_watanabe <- t(t(polars_slice) %*% normalized_matrix.fuman_rsvd) %>% as.data.frame %>% mutate(pola= as.matrix(.)%% c(-1,-1,-1,-1,-1,-1,-1,1,1,1,1,1)/2) %>%  select(pola) %>%  mutate(polarity = rowSums(.)) %>% one_to_minus_one
# autodic_watanabe
# dim(autodic_watanabe)

# autodic_watanabe <- t(t(polars_slice) %*% normalized_matrix.fuman_rsvd) %>% as.data.frame %>% mutate(polarity= as.matrix(.)%% c(-1,-1,-1,-1,-1,-1,-1,1,1,1,1,1)/12) %>%  select(polarity) %>% one_to_minus_one
# autodic_watanabe

autodic_watanabe <- t(t(polars_slice) %*% normalized_matrix.fuman_lsa) %>% 
  as.data.frame() %>% 
  mutate(across(1:7, ~ . * -1)) %>% 
  mutate(across(8:13, ~ . * 1)) %>% 
  mutate(polarity = rowMeans(.))

word <- list("最悪","良い","素敵","心配","コーヒー")


result <- autodic_watanabe[c("最悪","良い","素敵","心配","コーヒー"),] %>% as.data.frame()
result 
                # autodic_watanabe["悪い",]
#                autodic_watanabe["良い",],
#                autodic_watanabe["素敵",],
#                autodic_watanabe["心配",],
#                autodic_watanabe["コーヒー",])
install.packages("gt")
library(gt)
fig1<-result %>% gt()
```


# polarity value of the document
```{r}

# テキストを形態素解析
text1 <- RMeCabFreq("/home/mkobayashi/taniyama/fuman/test_fuman.csv_1/fuman_csv_1_1000/row_2_1.csv")
str(text1)

# get autodic_watanabe row names
row_names <- rownames(autodic_watanabe)

# 感情極性値を格納する変数を初期化
sentiment_values <- numeric()

# 各単語の感情極性値を計算
for (word in text1$Term) {
  # wordがautodic_watanabeに存在するか確認
  if (word %in% row_names) {
    # 単語の感情極性値を取得
    row_index <- which(row_names == word)  # wordが存在する行のインデックスを取得
    polarity <- autodic_watanabe[row_index, "polarity"]  # 行 index に対応するpolarity列の値を取得
    
    # 単語の出現回数を取得
    word_count <- text1$Freq[text1$Term == word]
    
    # 頻出度で重み付け
    weighted_polarity <- polarity * word_count
    sentiment_values <- c(sentiment_values, weighted_polarity)
  } else {
    # 単語がautodic_watanabeに存在しない場合、感情極性値を0とする
    sentiment_values <- c(sentiment_values, 0)
  }
}
sentiment_values 

# 文章全体の感情極性値を計算
sentiment_score <- sum(sentiment_values) / sum(text1$Freq)

# 結果を出力
cat("文章全体の感情極性値:", sentiment_score, "\n")

```



```{r}
# autodic_watanabeから行名を取得
row_names <- rownames(autodic_watanabe)

# 感情極性値を格納するリストを初期化
sentiment_values_list <- list()

# ファイル名のリストを作成
file_names <- paste0("/home/mkobayashi/taniyama/fuman/test_fuman.csv_1/fuman_csv_1_1000/row_", 1:5, "_1.csv")

# 各テキストファイルを処理
for (file_name in file_names) {
  # テキストを形態素解析
  text <- RMeCabFreq(file_names)
  
  # 各単語の感情極性値を計算
  sentiment_values <- numeric()
  for (word in text$Term) {
    if (word %in% row_names) {
      row_index <- which(row_names == word)
      polarity <- autodic_watanabe[row_index, "polarity"]
      word_count <- text$Freq[text$Term == word]
      weighted_polarity <- polarity * word_count
      sentiment_values <- c(sentiment_values, weighted_polarity)
    } else {
      sentiment_values <- c(sentiment_values, 0)
    }
  }
  
  # 文章全体の感情極性値を計算
  sentiment_score <- sum(sentiment_values) / sum(text$Freq)
  
  # 結果をリストに追加
  sentiment_values_list[[file_name]] <- sentiment_score
}


# 結果を出力
for (i in 1:10) {
  cat("文章", i, "の感情極性値:", sentiment_values_list[[file_names[i]]], "\n")
}
}

```

